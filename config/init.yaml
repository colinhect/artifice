# Example Artifice Configuration File (YAML)
# 
# Copy this file to ~/.config/artifice/init.yaml and customize as needed

# System Prompt
system_prompt: ""
prompt_prefix: ""

# Agent Configuration
agent: "minimax"
agents:

  # Huggingface models
  minimax:
    api_key: "..."
    provider: "openai"
    model: "MiniMaxAI/MiniMax-M2.5"
    base_url: "https://router.huggingface.co/v1"
    context_window: 200000
    tools:
      - "*"  # Enable all tools (supports fnmatch wildcards: "web_*", "python", etc.)

  # Ollama models (run locally via https://ollama.com)
  llama3:
    provider: "openai"
    model: "llama3.1"
    base_url: "http://localhost:11434/v1"
    api_key: "ollama"  # Ollama doesn't require a real key, but the field is needed
    context_window: 128000
    tools:
      - "*"

# Show/hide tool call output (the result blocks after tool execution)
# When false, output is still sent to the agent but hidden from the UI
show_tool_output: true

# Auto-send execution results to the agent
# When True, code execution results are automatically sent to the AI agent
send_user_commands_to_agent: true

# Display Settings
agent_markdown: true
python_markdown: false
shell_markdown: false

# TMUX integration
tmux_target: "session:window.pane"
tmux_prompt_pattern: '^\S+@\S+:\S+\$ '

# Performance Settings
# Adjust these values to fine-tune responsiveness vs. CPU usage
# Lower values = more responsive but higher CPU usage

# Streaming FPS: Target frames per second for UI updates during AI response streaming
# Default: 60 (16.6ms between updates) | Conservative: 30 (33ms)
streaming_fps: 60

# Shell poll interval: Seconds between checks for shell command output
# Default: 0.02 (20ms) | Conservative: 0.05 (50ms)
shell_poll_interval: 0.02

# Python executor sleep: Seconds to sleep between output checks during Python execution
# Default: 0.005 (5ms) | Conservative: 0.01 (10ms)
python_executor_sleep: 0.005
